"""
Setup script for local LLM models
Configures Phi-3, Qwen2, and other local models for use with CrewAI + LlamaIndex
"""

import subprocess
import sys
import os
from pathlib import Path

# Fix Windows console encoding
if sys.platform == 'win32':
    try:
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except Exception:
        pass


def print_header(text: str) -> None:
    """Print a formatted header."""
    print(f"\n{'='*60}")
    print(f"  {text}")
    print('='*60 + '\n')


def install_packages():
    """Install required packages for local models."""
    print_header("ğŸ“¦ Installing Required Packages")

    packages = [
        'llama-index-llms-ollama',
        'llama-index-embeddings-huggingface',
        'sentence-transformers',
        'torch',  # Required for local embeddings
        'transformers',  # Additional support
    ]

    for package in packages:
        print(f"ğŸ“¥ Installing {package}...")
        try:
            subprocess.run(
                [sys.executable, '-m', 'pip', 'install', package, '--quiet'],
                check=True,
                capture_output=True
            )
            print(f"âœ… {package} installed")
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸  Failed to install {package}: {e}")

    print("\nâœ… Package installation complete")


def check_ollama() -> bool:
    """Check if Ollama is installed and running."""
    print_header("ğŸ” Checking Ollama Installation")

    try:
        result = subprocess.run(
            ['ollama', '--version'],
            capture_output=True,
            text=True,
            timeout=5
        )
        print(f"âœ… Ollama installed: {result.stdout.strip()}")
        return True
    except (FileNotFoundError, subprocess.TimeoutExpired):
        print("âŒ Ollama not found!")
        print("\nğŸ“¥ Please install Ollama:")
        print("   Option 1: winget install Ollama.Ollama")
        print("   Option 2: Download from https://ollama.ai/download")
        print("\nAfter installing, run this script again.")
        return False


def list_available_models() -> list:
    """List currently available Ollama models."""
    try:
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')[1:]  # Skip header
            models = [line.split()[0] for line in lines if line.strip()]
            return models
        return []
    except Exception:
        return []


def pull_recommended_models():
    """Pull recommended models for legal research."""
    print_header("ğŸ“¥ Downloading Recommended Models")

    # Check what's already available
    existing_models = list_available_models()
    print(f"ğŸ“‹ Currently installed: {', '.join(existing_models) if existing_models else 'None'}\n")

    recommended = {
        'phi3:mini': 'Phi-3 Mini (Best for legal - 128K context)',
        'llama3.2:latest': 'Llama 3.2 (Alternative option)',
        'qwen2.5:1.5b': 'Qwen 2.5 (Fast and efficient)',
    }

    print("Recommended models for legal research:")
    for i, (model, desc) in enumerate(recommended.items(), 1):
        status = "âœ… Installed" if model in existing_models else "â¬‡ï¸  Download"
        print(f"  {i}. {model} - {desc} [{status}]")

    print("\n" + "-"*60)
    choice = input("\nDownload Phi-3 Mini now? (recommended) [Y/n]: ").strip().lower()

    if choice in ['', 'y', 'yes']:
        print("\nğŸ“¥ Pulling Phi-3 Mini (this may take a few minutes)...")
        try:
            subprocess.run(['ollama', 'pull', 'phi3:mini'], check=True)
            print("âœ… Phi-3 Mini downloaded successfully!")
        except subprocess.CalledProcessError:
            print("âŒ Failed to download Phi-3")
            print("   You can download it later with: ollama pull phi3:mini")
    else:
        print("\nğŸ’¡ Skipped. Download later with: ollama pull phi3:mini")


def test_local_llm():
    """Test local LLM connection."""
    print_header("ğŸ§ª Testing Local LLM")

    try:
        from llama_index.llms.ollama import Ollama

        print("ğŸ”Œ Connecting to Ollama...")
        llm = Ollama(model="phi3:mini", request_timeout=30.0)

        print("ğŸ’¬ Sending test query...")
        response = llm.complete("Respond with exactly: 'Local LLM working!'")

        print(f"ğŸ“ Response: {response.text}")
        print("âœ… Local LLM is working!\n")
        return True

    except Exception as e:
        print(f"âŒ LLM test failed: {e}")
        print("\nğŸ’¡ Troubleshooting:")
        print("   1. Make sure Ollama is running: ollama serve")
        print("   2. Check if model is pulled: ollama list")
        print("   3. Try pulling: ollama pull phi3:mini")
        return False


def test_local_embeddings():
    """Test local embeddings."""
    print_header("ğŸ§ª Testing Local Embeddings")

    try:
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding

        print("ğŸ“¥ Loading BGE embedding model (first time may download)...")
        embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-small-en-v1.5"
        )

        print("ğŸ”¢ Creating test embedding...")
        embedding = embed_model.get_text_embedding("This is a test sentence.")

        print(f"âœ… Embeddings working! (Dimension: {len(embedding)})\n")
        return True

    except Exception as e:
        print(f"âŒ Embeddings test failed: {e}")
        print("\nğŸ’¡ Try reinstalling: pip install sentence-transformers")
        return False


def check_local_models():
    """Check for locally cached models."""
    print_header("ğŸ“‚ Checking Local Model Cache")

    models_dir = Path("models_cache")
    if not models_dir.exists():
        print("âš ï¸  models_cache directory not found")
        return

    found_models = []
    for model_dir in models_dir.iterdir():
        if model_dir.is_dir():
            found_models.append(model_dir.name)

    if found_models:
        print("âœ… Found local models:")
        for model in found_models:
            print(f"   â€¢ {model}")
        print("\nğŸ’¡ These can be used with transformers/HuggingFace")
    else:
        print("âš ï¸  No local models found in models_cache/")


def create_config_file():
    """Create a configuration file for local models."""
    print_header("ğŸ“ Creating Configuration File")

    config = """# Local Models Configuration
# Generated by setup_local_models.py

[llm]
# Primary model for reasoning
provider = ollama
model = phi3:mini
temperature = 0.1
request_timeout = 120

[embeddings]
# Local embedding model
provider = huggingface
model = BAAI/bge-small-en-v1.5

[ollama]
# Ollama server settings
base_url = http://localhost:11434
timeout = 120

[storage]
# Where to store indices
index_dir = ./storage/local_indices
cache_dir = ./cache

[performance]
# Performance tuning
chunk_size = 1024
chunk_overlap = 100
similarity_top_k = 5
"""

    config_path = Path("local_models_config.ini")
    config_path.write_text(config)
    print(f"âœ… Configuration saved to: {config_path}")
    print("   You can edit this file to customize settings")


def create_example_script():
    """Create an example script using local models."""
    print_header("ğŸ“ Creating Example Script")

    example = '''"""
Example: Using Local Models for Legal Research
NO API COSTS - Completely free and private!
"""

from llama_index.llms.ollama import Ollama
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

def main():
    print("ğŸš€ Local Legal Research Example\\n")

    # Configure local models
    print("âš™ï¸  Configuring local models...")
    Settings.llm = Ollama(
        model="phi3:mini",
        request_timeout=120.0,
        temperature=0.1
    )
    Settings.embed_model = HuggingFaceEmbedding(
        model_name="BAAI/bge-small-en-v1.5"
    )

    # Load a test document
    print("ğŸ“š Loading documents...")
    # Replace with your actual document directory
    documents = SimpleDirectoryReader('1782 Case PDF Database').load_data()
    print(f"âœ… Loaded {len(documents)} documents")

    # Create index
    print("ğŸ”¨ Creating vector index...")
    index = VectorStoreIndex.from_documents(documents[:5])  # Start with 5 docs

    # Query the index
    print("ğŸ” Querying with local models...\\n")
    query_engine = index.as_query_engine(similarity_top_k=3)

    question = "What are the Intel factors in 1782 applications?"
    print(f"â“ Question: {question}\\n")

    response = query_engine.query(question)
    print(f"ğŸ“ Answer:\\n{response}\\n")

    print("âœ… Local research complete - no API costs!")

if __name__ == "__main__":
    main()
'''

    example_path = Path("local_legal_research_example.py")
    example_path.write_text(example)
    print(f"âœ… Example saved to: {example_path}")
    print("   Run it with: python local_legal_research_example.py")


def print_summary(llm_ok: bool, embed_ok: bool):
    """Print setup summary."""
    print_header("ğŸ“Š Setup Summary")

    print("Component Status:")
    print(f"  â€¢ Ollama: {'âœ… Working' if llm_ok else 'âŒ Not working'}")
    print(f"  â€¢ Local LLM: {'âœ… Working' if llm_ok else 'âŒ Not working'}")
    print(f"  â€¢ Embeddings: {'âœ… Working' if embed_ok else 'âŒ Not working'}")

    if llm_ok and embed_ok:
        print("\nğŸ‰ All systems ready for local research!")
        print("\nğŸ“š Next Steps:")
        print("   1. Review: LOCAL_MODELS_SETUP_GUIDE.md")
        print("   2. Run: python local_legal_research_example.py")
        print("   3. Start researching with NO API costs!")
        print("\nğŸ’° Cost savings: 100% (vs OpenAI API)")
        print("ğŸ”’ Privacy: 100% (all data stays local)")
    else:
        print("\nâš ï¸  Some components need attention")
        print("\nğŸ’¡ Troubleshooting:")
        if not llm_ok:
            print("   â€¢ Install Ollama: https://ollama.ai/download")
            print("   â€¢ Pull model: ollama pull phi3:mini")
        if not embed_ok:
            print("   â€¢ Install: pip install sentence-transformers torch")


def main():
    """Main setup process."""
    print("\nğŸ¤– Local LLM Models Setup for Legal Research")
    print("   Using: Phi-3, Qwen, Llama (your downloaded models)")

    # Step 1: Install packages
    install_packages()

    # Step 2: Check Ollama
    ollama_ok = check_ollama()
    if not ollama_ok:
        print("\nâ¸ï¸  Setup paused. Install Ollama and run again.")
        return

    # Step 3: Check/download models
    pull_recommended_models()

    # Step 4: Check local cache
    check_local_models()

    # Step 5: Test LLM
    llm_ok = test_local_llm()

    # Step 6: Test embeddings
    embed_ok = test_local_embeddings()

    # Step 7: Create config
    create_config_file()

    # Step 8: Create example
    create_example_script()

    # Step 9: Summary
    print_summary(llm_ok, embed_ok)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Setup interrupted by user")
    except Exception as e:
        print(f"\nâŒ Setup failed: {e}")
        import traceback
        traceback.print_exc()

