#!/usr/bin/env python3
"""
Universal Agent-Database Integration

Integrates BOTH agent teams (49 writing team + ML system) with BOTH databases
(lawsuit MySQL + 1782 SQLite) for comprehensive cross-database capabilities.
"""

import json
import uuid
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

def load_1782_memory_seeds():
    """Load 1782 memory seeds."""
    with open("config/1782_memory_seeds.json", 'r') as f:
        return json.load(f)

def get_writing_team_agents():
    """Get the 49 writing team agents from agent_context_templates.py"""
    return [
        # Citation Pipeline (5 agents)
        "CitationFinderAgent", "CitationNormalizerAgent", "CitationVerifierAgent",
        "CitationLocatorAgent", "CitationInserterAgent",

        # Research Pipeline (6 agents)
        "FactExtractorAgent", "PrecedentFinderAgent", "PrecedentRankerAgent",
        "PrecedentSummarizerAgent", "StatuteLocatorAgent", "ExhibitFetcherAgent",

        # Writing Pipeline (5 agents)
        "OutlineBuilderAgent", "SectionWriterAgent", "ParagraphWriterAgent",
        "TransitionAgent", "PrimaryWriterAgent",

        # Editing Pipeline (5 agents)
        "GrammarFixerAgent", "StyleCheckerAgent", "LogicCheckerAgent",
        "ConsistencyCheckerAgent", "RedactionAgent",

        # Quality Pipeline (5 agents)
        "ComplianceAgent", "ExpertQAAgent", "ContentReviewerAgent",
        "TechnicalReviewerAgent", "StyleReviewerAgent",

        # Export Pipeline (3 agents)
        "MarkdownExporterAgent", "DocxExporterAgent", "MetadataTaggerAgent",

        # Planning Pipeline (3 agents)
        "StrategicPlannerAgent", "TacticalPlannerAgent", "OperationalPlannerAgent",

        # Core Pipeline (3 agents)
        "ResearchAgent", "QualityAssuranceAgent", "AdaptiveOrchestratorAgent",

        # Autogen Pipeline (3 agents)
        "PlannerAgent", "WriterAgent", "EditorAgent",

        # Specialized Pipeline (3 agents)
        "DoubleCheckerAgent", "StylistAgent", "WriterAgent (autogen)",

        # Settlement Pipeline (4 agents)
        "SettlementOptimizerAgent", "BATNAAnalyzerAgent",
        "NashEquilibriumCalculatorAgent", "ReputationRiskScorerAgent",

        # Analysis Pipeline (4 agents)
        "TimelineAgent", "LinguistAgent", "CausalAgent", "LegalAgent"
    ]

def get_ml_system_agents():
    """Get the ML system agents"""
    return [
        "UniversalAgent", "OutcomePredictorAgent", "DocumentClassifierAgent",
        "PatternRecognizerAgent", "LegalResearchAgent", "CaseAnalysisAgent",
        "CitationAnalysisAgent", "TextAnalysisAgent", "StatisticalAnalysisAgent",
        "ReportGenerationAgent", "MemoryManagementAgent"
    ]

def create_writing_team_1782_memories():
    """Create 1782-specific memories for the 49 writing team agents."""
    print("üîÑ Creating 1782 Memories for 49 Writing Team Agents...")

    writing_agents = get_writing_team_agents()
    memory_seeds = load_1782_memory_seeds()

    all_memories = []

    # Universal 1782 skills for all writing agents
    universal_skills = memory_seeds['universal_skills']
    for agent_name in writing_agents:
        for skill in universal_skills:
            memory_obj = {
                "agent_type": agent_name,
                "memory_id": str(uuid.uuid4()),
                "summary": f"1782 Universal Skill: {skill['id']} - {skill['nl']}",
                "context": {
                    "skill_id": skill['id'],
                    "question": skill['nl'],
                    "priority": skill['priority'],
                    "category": skill['category'],
                    "source": "1782_analysis",
                    "database": "1782_pdf_analysis.db",
                    "integration_date": datetime.now().isoformat(),
                    "agent_role": "writing_team"
                },
                "source": "1782_capability_seed",
                "timestamp": datetime.now().isoformat(),
                "relevance_score": 0.0
            }
            all_memories.append(memory_obj)

    # Agent-specific 1782 capabilities for writing agents
    for agent_name in writing_agents:
        # Map writing agents to relevant 1782 capabilities
        if "Citation" in agent_name:
            capabilities = ["1782_citation_analysis", "1782_precedent_tracking"]
        elif "Precedent" in agent_name:
            capabilities = ["1782_precedent_analysis", "1782_legal_research"]
        elif "Writer" in agent_name or "Section" in agent_name or "Paragraph" in agent_name:
            capabilities = ["1782_text_analysis", "1782_report_generation"]
        elif "Research" in agent_name or "Fact" in agent_name:
            capabilities = ["1782_case_analysis", "1782_legal_research"]
        elif "Review" in agent_name or "Quality" in agent_name:
            capabilities = ["1782_outcome_analysis", "1782_success_patterns"]
        elif "Legal" in agent_name:
            capabilities = ["1782_legal_research", "1782_precedent_analysis"]
        elif "Settlement" in agent_name or "BATNA" in agent_name or "Nash" in agent_name:
            capabilities = ["1782_outcome_prediction", "1782_success_probability"]
        else:
            capabilities = ["1782_outcome_analysis", "1782_case_analysis"]

        for capability in capabilities:
            memory_obj = {
                "agent_type": agent_name,
                "memory_id": str(uuid.uuid4()),
                "summary": f"1782 Writing Team Capability: {capability} - Enhanced {agent_name} with 1782 case law analysis",
                "context": {
                    "capability_id": capability,
                    "question": f"How can {agent_name} use 1782 case law analysis in their workflow?",
                    "priority": 2,
                    "category": "1782_writing_integration",
                    "source": "1782_analysis",
                    "database": "1782_pdf_analysis.db",
                    "integration_date": datetime.now().isoformat(),
                    "agent_role": "writing_team"
                },
                "source": "1782_capability_seed",
                "timestamp": datetime.now().isoformat(),
                "relevance_score": 0.0
            }
            all_memories.append(memory_obj)

    return all_memories

def create_ml_team_lawsuit_memories():
    """Create lawsuit database memories for ML system agents."""
    print("üîÑ Creating Lawsuit Database Memories for ML System Agents...")

    ml_agents = get_ml_system_agents()

    # Lawsuit database capabilities for ML agents
    lawsuit_capabilities = [
        {
            "id": "lawsuit_schema_discovery",
            "nl": "List all tables in lawsuit database; for each, show 5 rows and column names",
            "category": "lawsuit_analysis"
        },
        {
            "id": "lawsuit_document_search",
            "nl": "Search lawsuit documents for specific legal terms and concepts",
            "category": "lawsuit_analysis"
        },
        {
            "id": "lawsuit_citation_patterns",
            "nl": "Analyze citation patterns in lawsuit documents",
            "category": "lawsuit_analysis"
        },
        {
            "id": "lawsuit_outcome_analysis",
            "nl": "Analyze lawsuit outcomes and success patterns",
            "category": "lawsuit_analysis"
        },
        {
            "id": "lawsuit_text_analysis",
            "nl": "Perform text analysis on lawsuit documents",
            "category": "lawsuit_analysis"
        },
        {
            "id": "lawsuit_statistical_analysis",
            "nl": "Perform statistical analysis on lawsuit data",
            "category": "lawsuit_analysis"
        }
    ]

    all_memories = []

    for agent_name in ml_agents:
        for capability in lawsuit_capabilities:
            memory_obj = {
                "agent_type": agent_name,
                "memory_id": str(uuid.uuid4()),
                "summary": f"Lawsuit Database Capability: {capability['id']} - {capability['nl']}",
                "context": {
                    "capability_id": capability['id'],
                    "question": capability['nl'],
                    "priority": 2,
                    "category": capability['category'],
                    "source": "lawsuit_analysis",
                    "database": "lawsuit_docs (MySQL)",
                    "integration_date": datetime.now().isoformat(),
                    "agent_role": "ml_system"
                },
                "source": "lawsuit_capability_seed",
                "timestamp": datetime.now().isoformat(),
                "relevance_score": 0.0
            }
            all_memories.append(memory_obj)

    return all_memories

def create_cross_database_capabilities():
    """Create cross-database query capabilities for both teams."""
    print("üîÑ Creating Cross-Database Capabilities...")

    writing_agents = get_writing_team_agents()
    ml_agents = get_ml_system_agents()
    all_agents = writing_agents + ml_agents

    cross_database_capabilities = [
        {
            "id": "cross_db_comparison",
            "nl": "Compare patterns between lawsuit database and 1782 database",
            "category": "cross_database"
        },
        {
            "id": "cross_db_citation_analysis",
            "nl": "Analyze citation patterns across both lawsuit and 1782 databases",
            "category": "cross_database"
        },
        {
            "id": "cross_db_outcome_correlation",
            "nl": "Find correlations between lawsuit outcomes and 1782 case outcomes",
            "category": "cross_database"
        },
        {
            "id": "cross_db_precedent_tracking",
            "nl": "Track precedent citations across both databases",
            "category": "cross_database"
        },
        {
            "id": "cross_db_text_patterns",
            "nl": "Identify text patterns common to both lawsuit and 1782 documents",
            "category": "cross_database"
        }
    ]

    all_memories = []

    for agent_name in all_agents:
        for capability in cross_database_capabilities:
            memory_obj = {
                "agent_type": agent_name,
                "memory_id": str(uuid.uuid4()),
                "summary": f"Cross-Database Capability: {capability['id']} - {capability['nl']}",
                "context": {
                    "capability_id": capability['id'],
                    "question": capability['nl'],
                    "priority": 1,
                    "category": capability['category'],
                    "source": "cross_database_analysis",
                    "databases": ["lawsuit_docs (MySQL)", "1782_pdf_analysis.db (SQLite)"],
                    "integration_date": datetime.now().isoformat(),
                    "agent_role": "universal"
                },
                "source": "cross_database_capability_seed",
                "timestamp": datetime.now().isoformat(),
                "relevance_score": 0.0
            }
            all_memories.append(memory_obj)

    return all_memories

def main():
    """Main execution function."""
    print("üöÄ Universal Agent-Database Integration")
    print("="*60)

    # Create memory store directory
    memory_dir = Path("memory_store")
    memory_dir.mkdir(exist_ok=True)

    # Step 1: Writing team 1782 memories
    writing_1782_memories = create_writing_team_1782_memories()

    # Step 2: ML team lawsuit memories
    ml_lawsuit_memories = create_ml_team_lawsuit_memories()

    # Step 3: Cross-database capabilities
    cross_db_memories = create_cross_database_capabilities()

    # Combine all memories
    all_memories = writing_1782_memories + ml_lawsuit_memories + cross_db_memories

    # Save all memories
    with open("memory_store/universal_integration_memories.json", 'w') as f:
        json.dump(all_memories, f, indent=2)

    # Create comprehensive summary
    summary = {
        "integration_date": datetime.now().isoformat(),
        "total_memories": len(all_memories),
        "agent_teams": {
            "writing_team": {
                "count": len(get_writing_team_agents()),
                "1782_memories": len(writing_1782_memories),
                "cross_db_memories": len([m for m in cross_db_memories if m['agent_type'] in get_writing_team_agents()])
            },
            "ml_system": {
                "count": len(get_ml_system_agents()),
                "lawsuit_memories": len(ml_lawsuit_memories),
                "cross_db_memories": len([m for m in cross_db_memories if m['agent_type'] in get_ml_system_agents()])
            }
        },
        "databases": {
            "lawsuit_docs": {
                "type": "MySQL",
                "description": "General lawsuit database",
                "access": "Both teams"
            },
            "1782_pdf_analysis": {
                "type": "SQLite",
                "description": "1782 case law analysis database",
                "access": "Both teams"
            }
        },
        "capabilities": {
            "writing_team": [
                "1782 case law analysis", "Cross-database queries",
                "Enhanced citation analysis", "1782 precedent tracking",
                "1782 outcome prediction", "Cross-database pattern recognition"
            ],
            "ml_system": [
                "Lawsuit document analysis", "Cross-database queries",
                "Enhanced ML analysis", "Lawsuit pattern recognition",
                "Cross-database correlation analysis"
            ]
        }
    }

    with open("memory_store/universal_integration_summary.json", 'w') as f:
        json.dump(summary, f, indent=2)

    # Print results
    print(f"\n‚úÖ Universal Integration Complete!")
    print(f"="*60)
    print(f"üìä Integration Summary:")
    print(f"   Total Memories Created: {len(all_memories)}")
    print(f"   Writing Team Agents: {len(get_writing_team_agents())}")
    print(f"   ML System Agents: {len(get_ml_system_agents())}")
    print(f"   Writing Team 1782 Memories: {len(writing_1782_memories)}")
    print(f"   ML Team Lawsuit Memories: {len(ml_lawsuit_memories)}")
    print(f"   Cross-Database Memories: {len(cross_db_memories)}")

    print(f"\nüóÑÔ∏è Database Access:")
    print(f"   Lawsuit Database (MySQL): ‚úÖ Both Teams")
    print(f"   1782 Database (SQLite): ‚úÖ Both Teams")

    print(f"\nüéØ Capabilities:")
    print(f"   Writing Team: 1782 analysis + Cross-database queries")
    print(f"   ML System: Lawsuit analysis + Cross-database queries")
    print(f"   Both Teams: Full cross-database pattern recognition")

    print(f"\nüìÅ Files Created:")
    print(f"   memory_store/universal_integration_memories.json")
    print(f"   memory_store/universal_integration_summary.json")

    print(f"\nüß† Integration Status: COMPLETE")
    print(f"üéØ Both agent teams now have access to both databases!")

if __name__ == "__main__":
    main()
