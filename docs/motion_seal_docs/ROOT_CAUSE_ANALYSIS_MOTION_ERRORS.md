t# Root Cause Analysis: Motion Generation Errors

## Executive Summary

The motion generation system produced **extensive factual errors** including:
- False claims (public figure, hacking, illegal activities, 2018 timeline)
- Fabricated case law citations
- Generic privacy law content unrelated to the case
- Hallucinated evidence (affidavits, threat logs, nodes)

This document identifies the **root causes** and **fixes needed**.

---

## üî¥ ROOT CAUSE #1: AutoGen Exploration Notes Contamination

### Problem
**AutoGen exploration phase generates hallucinated facts that contaminate motion generation.**

**How it happens:**
1. AutoGen exploration agent receives facts and is asked to "explore arguments"
2. AutoGen generates notes that may include **speculative or invented facts**
3. These notes are passed **directly** to motion generation prompt as "AUTOGEN EXPLORATION NOTES"
4. Motion generator sees these notes and treats them as valid facts
5. Result: Hallucinated facts from exploration become part of the motion

**Code location:**
- `WorkflowOrchestrator.py:3288-3319` - Exploration phase
- `WorkflowOrchestrator.py:3756-3785` - AutoGen notes included in prompt
- `WorkflowOrchestrator.py:625-634` - Notes formatting (no filtering)

**Evidence:**
```python
# Line 3783-3785: AutoGen notes included without validation
AUTOGEN EXPLORATION NOTES:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
{autogen_notes_section}
```

**Impact:** HIGH - AutoGen can invent entire timelines, events, and facts that get written into the motion.

---

## üî¥ ROOT CAUSE #2: No Post-Generation Fact Validation

### Problem
**Generated content is NOT fact-checked against the database before being written to Google Doc.**

**How it happens:**
1. LLM generates motion sections
2. Validation checks format/structure (word count, citations, tone)
3. **NO validation checks if facts in the motion exist in the database**
4. Motion is written to Google Doc with hallucinated facts

**Code location:**
- `WorkflowOrchestrator.py:1440-1503` - `_validate_content_is_actual_motion()` only checks format
- `WorkflowOrchestrator.py:1316-1365` - `_validate_personal_facts_coverage()` exists but may not catch all hallucinations
- No fact-by-fact validation against `fact_registry` database

**Evidence:**
```python
# Line 1440-1503: Validation only checks for:
# - Test patterns ("your task:", etc.)
# - Motion indicators ("respectfully requests", etc.)
# - Does NOT check if facts exist in database
```

**Impact:** CRITICAL - System cannot detect when LLM invents facts not in the database.

---

## üî¥ ROOT CAUSE #3: Section-by-Section Generation Loses Context

### Problem
**Motion sections are generated separately, allowing each section to hallucinate independently.**

**How it happens:**
1. System generates Introduction section ‚Üí LLM may invent timeline
2. System generates Factual Background section ‚Üí LLM may invent different timeline
3. System generates Legal Standard section ‚Üí LLM may invent case law
4. Sections are combined without cross-checking consistency
5. Result: Contradictory facts across sections

**Code location:**
- `WorkflowOrchestrator.py:4865-4950` - `_generate_complete_motion_from_scratch()`
- `WorkflowOrchestrator.py:4576-4690` - `_generate_all_motion_sections()`
- Each section generated independently with same prompt

**Impact:** MEDIUM-HIGH - Allows inconsistent hallucinations across sections.

---

## üî¥ ROOT CAUSE #4: Research Findings May Contain False Citations

### Problem
**Case law research may return irrelevant cases or LLM may hallucinate case names.**

**How it happens:**
1. Research phase finds similar cases
2. Research findings passed to motion generation
3. LLM sees case names and may:
   - Misapply them
   - Invent similar-sounding case names
   - Use citations without proper context
4. No validation that cited cases actually exist or are relevant

**Code location:**
- `WorkflowOrchestrator.py:3724-3751` - Research findings formatting
- `WorkflowOrchestrator.py:3738-3742` - Case names included without validation

**Evidence:**
```python
# Line 3738-3742: Case names included without fact-checking
for i, case in enumerate(cases[:5], 1):
    case_name = case.get('case_name', 'Unknown')
    court = case.get('court', 'Unknown')
    similarity = case.get('similarity_score', 0.0)
    research_section += f"{i}. {case_name} ({court}) - Relevance: {similarity:.2f}\n"
```

**Impact:** MEDIUM - False case citations can be legally damaging.

---

## üî¥ ROOT CAUSE #5: Prompt Confusion - Multiple "Authoritative" Sources

### Problem
**Prompt includes multiple sections that may conflict, confusing the LLM about which facts are authoritative.**

**How it happens:**
1. Prompt includes:
   - STRUCTURED FACTS (authoritative)
   - EVIDENCE (authoritative)
   - AUTOGEN EXPLORATION NOTES (may contain hallucinations)
   - RESEARCH FINDINGS (may contain irrelevant cases)
   - CASE SUMMARY (may be generic)
2. LLM sees conflicting information and may:
   - Prefer AutoGen notes over structured facts
   - Mix facts from different sources
   - Invent facts to "fill gaps"

**Code location:**
- `WorkflowOrchestrator.py:3764-3817` - Full prompt construction
- Multiple sections without clear hierarchy

**Impact:** MEDIUM - LLM may prioritize wrong sources.

---

## üî¥ ROOT CAUSE #6: LLM Training Data Contamination

### Problem
**LLM (qwen2.5:14b) was trained on generic legal templates and may default to those patterns.**

**How it happens:**
1. LLM sees prompt about "motion to seal"
2. LLM recalls training data patterns:
   - Generic privacy law templates
   - Common defamation case structures
   - Typical "public figure" arguments
3. LLM generates content matching training patterns instead of using provided facts
4. Result: Generic legal content unrelated to your specific case

**Evidence:**
- Motion contains generic privacy law (Intrusion Upon Seclusion, etc.)
- Motion uses stock phrases ("public figure", "political adversaries")
- Motion includes common defamation templates

**Impact:** HIGH - LLM defaults to generic templates instead of case-specific facts.

---

## üî¥ ROOT CAUSE #7: No Fact Extraction from Generated Content

### Problem
**System doesn't extract facts from generated motion and verify them against database.**

**How it happens:**
1. Motion is generated
2. System validates format/structure
3. **System does NOT:**
   - Extract factual claims from motion
   - Check each claim against `fact_registry` database
   - Flag claims that don't exist in database
   - Reject motion if too many false claims

**Code location:**
- No function to extract facts from generated text
- No function to fact-check extracted claims against database
- Validation only checks format, not factual accuracy

**Impact:** CRITICAL - System cannot detect factual errors.

---

## üî¥ ROOT CAUSE #8: Structured Facts May Be Incomplete or Poorly Formatted

### Problem
**Structured facts from database may not be formatted clearly enough for LLM to use.**

**How it happens:**
1. Facts loaded from `fact_registry` database
2. Facts formatted into prompt (line 3755: `_format_structured_facts_block()`)
3. Formatting may:
   - Lose important context
   - Group facts in confusing ways
   - Omit critical details
4. LLM sees poorly formatted facts and invents better-sounding alternatives

**Code location:**
- `WorkflowOrchestrator.py:593-622` - `_format_structured_facts_block()`
- `CaseFactsProvider.py:format_facts_for_autogen()` - Fact formatting

**Impact:** MEDIUM - Poor formatting may cause LLM to ignore real facts.

---

## üî¥ ROOT CAUSE #9: Validation Runs But Doesn't Block Commit

### Problem
**Personal facts verifier detects violations but system commits anyway.**

**How it happens:**
1. Validation detects violations/contradictions
2. System logs warnings
3. **System still commits motion to Google Doc**
4. Warnings are ignored

**Code location:**
- `WorkflowOrchestrator.py:1316-1365` - Validation returns warnings
- `WorkflowOrchestrator.py:860-870` - System commits even with weak features
- No hard stop for factual violations

**Evidence:**
```python
# Line 1360-1363: Warnings generated but motion still committed
if has_violations:
    warnings.append("Rejected prohibited facts detected in draft.")
if has_contradictions:
    warnings.append("Draft contains statements that contradict verified facts.")
# But motion is still committed!
```

**Impact:** HIGH - System knows about errors but doesn't prevent them.

---

## üî¥ ROOT CAUSE #10: No Fact Grounding Enforcement

### Problem
**System doesn't enforce that every factual claim must be traceable to a database fact.**

**How it happens:**
1. LLM generates motion with factual claims
2. System doesn't require each claim to cite a fact_id
3. System doesn't verify claims match database facts
4. Result: LLM can make any claim without accountability

**Code location:**
- No fact grounding requirement in prompts
- No fact citation system
- No fact-by-fact verification

**Impact:** CRITICAL - No accountability for factual accuracy.

---

## üìä Summary: Why So Many Errors?

### Primary Causes (in order of impact):

1. **AutoGen notes contamination** - Hallucinated facts from exploration phase
2. **No post-generation fact validation** - System doesn't check if facts exist
3. **LLM training data contamination** - Defaults to generic templates
4. **No fact grounding enforcement** - Claims don't need to cite sources
5. **Validation doesn't block commit** - Warnings ignored
6. **Section-by-section generation** - Loses context, allows inconsistencies
7. **Research findings contamination** - False case citations
8. **Prompt confusion** - Multiple conflicting sources
9. **Poor fact formatting** - LLM may ignore real facts
10. **No fact extraction** - Can't verify claims against database

### The Cascade Effect:

```
AutoGen Exploration ‚Üí Generates false facts
         ‚Üì
AutoGen Notes ‚Üí Passed to motion generation
         ‚Üì
Motion Generation ‚Üí Sees false facts + generic templates
         ‚Üì
LLM Generates ‚Üí Mixes false facts + templates + real facts
         ‚Üì
Validation ‚Üí Checks format only, not facts
         ‚Üì
Commit ‚Üí Motion written with errors
```

---

## ‚úÖ Required Fixes

### Fix #1: Filter AutoGen Notes (CRITICAL)
- **Action:** Remove or heavily filter AutoGen exploration notes before passing to motion generation
- **Alternative:** Don't include AutoGen notes in motion generation prompt at all
- **Code:** `WorkflowOrchestrator.py:3756-3785`

### Fix #2: Add Post-Generation Fact Validation (CRITICAL)
- **Action:** After generation, extract all factual claims and verify against `fact_registry`
- **Action:** Reject motion if >5% of claims don't exist in database
- **Code:** New function needed

### Fix #3: Enforce Fact Grounding (CRITICAL)
- **Action:** Require every factual claim to cite a fact_id from database
- **Action:** Reject claims without citations
- **Code:** Modify prompt and validation

### Fix #4: Block Commit on Factual Violations (HIGH)
- **Action:** If validation detects violations/contradictions, DO NOT commit
- **Action:** Force regeneration or manual review
- **Code:** `WorkflowOrchestrator.py:860-870`

### Fix #5: Improve Fact Formatting (MEDIUM)
- **Action:** Format facts more clearly with fact_ids and source citations
- **Action:** Group facts by timeline/type for better LLM understanding
- **Code:** `_format_structured_facts_block()`

### Fix #6: Validate Case Citations (MEDIUM)
- **Action:** Verify all case citations exist in case law database
- **Action:** Reject motion if citations are invalid
- **Code:** New validation function

### Fix #7: Generate Motion in One Pass (MEDIUM)
- **Action:** Generate entire motion at once instead of section-by-section
- **Action:** This maintains context and reduces inconsistencies
- **Code:** `_generate_complete_motion_from_scratch()`

### Fix #8: Strengthen Prompt (LOW)
- **Action:** Make prompt even more explicit about using ONLY database facts
- **Action:** Add examples of what NOT to do
- **Code:** `WorkflowOrchestrator.py:3764-3817`

---

## üéØ Priority Order for Fixes

1. **IMMEDIATE:** Remove AutoGen notes from motion generation prompt
2. **IMMEDIATE:** Add post-generation fact validation
3. **HIGH:** Block commit on factual violations
4. **HIGH:** Enforce fact grounding with citations
5. **MEDIUM:** Improve fact formatting
6. **MEDIUM:** Validate case citations
7. **LOW:** Generate in one pass
8. **LOW:** Strengthen prompt

---

## üìù Next Steps

1. Implement Fix #1 (remove AutoGen notes)
2. Implement Fix #2 (post-generation fact validation)
3. Test with a new motion generation
4. Iterate on remaining fixes

